Model name,Use case,Model description,Owner,Version,Max allowed error rate for pipeline test,Settings for pipeline variables,Memory footprint,Training epochs,Training batch size,Training GPU,Intended users,Intended use cases,Out-of-scope use cases,Metrics,Evaluation Data,Training Data,Ethical Considerations,References
sid-minibert-20211021.onnx,sensitive-information-detection,"This model is a transformer-based sequence classifier (mini-bert) trained to detect sensitive data in unencrypted text. The ten categories of sensitive information include- address, bank account, credit card number, email address, government id number, full name, password, phone number, secret keys, and user names.",Rachel Allen,0.2.0,4%,for tokenizer: hash-file=bert-base-uncased max-length=256 stride=64 do-lower=TRUE do-truncation=FALSE,43MB,1,32,V100,cyber security and IT professionals,To detect leaked sensitive information from raw L7 payload data,This model version is designed for english language text data. It may not perform well on other languages.,F1=0.96,200k synthetic dataset with balanced classes for all 10 sensitive data labels, 2 million synethic pcap payloads generated using the faker repo to mimic sensitive and benign data found in nested jsons from web APIs and environmental variables,N/A,"Well-Read Students Learn Better: On the Importance of Pre-training Compact Models, 2019,  arXiv:1908.08962v2"
phishing-bert-20211006.onnx,phishing-email-detection,"This use case is currently implemeted to differentiate between phishing and non-phishing emails. The models for this use case are NLP models, specifically transformer-based models with attention (e.g., BERT).",Gorkem Batmaz,0.1.0,0.10%,for tokenizer: hash-file=bert-base-uncased max-length=128 stride=64 do-lower=TRUE do-truncation=FALSE,417MB,3,32,V100,cyber security and IT professionals,To detect phishing emails,This model version is designed for english language text data. It may not perform well on other languages.,F1=0.984,4946 labelled emails from three public datasets,19783 labelled emails from three public datasets,N/A,"Radev, D. (2008), CLAIR collection of fraud email, ACL Data and Code Repository, ADCR2008T001, http://aclweb.org/aclwiki
https://www.kaggle.com/rtatman/fraudulent-email-corpus *
https://www.cs.cmu.edu/~./enron/
https://spamassassin.apache.org/old/publiccorpus/readme.html
https://github.com/huggingface/transformers/tree/master/examples#
https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/
https://github.com/ThilinaRajapakse/pytorch-transformers-classification
https://mccormickml.com/2019/07/22/BERT-fine-tuning/"
abp-nvsmi-xgb-20210310.bst,anomalous-behavior-profiling,"This use case is currently implemeted to differentiate between crypto mining / GPU malware and other GPU-based workflows (e.g., ML/DL training). The model is a XGBoost model.",Gorkem Batmaz,0.1.1,TBD,N/A,3KB,5,N/,V100,cyber security and IT professionals,To detect crypto mining,,Accuracy=1,248 labelled nv-smi logs,994 labelled nv-smi logs,N/A,"https://docs.rapids.ai/api/cuml/stable/
https://medium.com/rapids-ai/rapids-forest-inference-library-prediction-at-100-million-rows-per-second-19558890bc35
https://developer.nvidia.com/nvidia-system-management-interface
https://developer.nvidia.com/morpheus-cybersecurity
https://github.com/rapidsai/clx/blob/branch-0.20/examples/forest_inference/xgboost_training.ipynb"
